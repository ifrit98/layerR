% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activation.R
\name{gelu}
\alias{gelu}
\title{Gaussian Error Linear Unit.
This is a smoother version of the RELU.
Original paper: https://arxiv.org/abs/1606.08415
Args:
  x: float Tensor to perform activation.
Returns:
 x with the GELU activation applied.}
\usage{
gelu(x)
}
\description{
Gaussian Error Linear Unit.
This is a smoother version of the RELU.
Original paper: https://arxiv.org/abs/1606.08415
Args:
  x: float Tensor to perform activation.
Returns:
 x with the GELU activation applied.
}
