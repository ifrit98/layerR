% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activation.R
\name{layer_activation_gelu}
\alias{layer_activation_gelu}
\title{keras lambda layer Gaussian Error Linear Unit.
This is a smoother version of the RELU.
Original paper: https://arxiv.org/abs/1606.08415
Args:
  x: float Tensor to perform activation.
Returns:
 x with the GELU activation applied.}
\usage{
layer_activation_gelu(object, name = "gelu")
}
\description{
keras lambda layer Gaussian Error Linear Unit.
This is a smoother version of the RELU.
Original paper: https://arxiv.org/abs/1606.08415
Args:
  x: float Tensor to perform activation.
Returns:
 x with the GELU activation applied.
}
